{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, GPy, seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE structure in linear data\n",
    "\n",
    "## by Max Zwiessele\n",
    "\n",
    "Recently a tweet https://twitter.com/mikelove/status/738021869341839360 shook data scientists awake, showing that one of the most used dimensionality reduction techniques stochastic neighberhood embedding is 'making up' structure in a linear embedding. Their plots show incredible detailed structure in a high dimensional dataset containing only a one dimensional linear embedded input. \n",
    "\n",
    "Here we explore the same problem using python's implementations of tSNE and show that GPLVM and Bayesian GPLVM do not suffer from this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "m = 40\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-1, 1, n)\n",
    "c = np.digitize(x, np.linspace(-1,1,12))-1\n",
    "cols = np.asarray(sns.color_palette('spectral_r',12))[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear embedding\n",
    "\n",
    "The embedded linear structure. It is just a point cloud on a line betwenn -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, np.zeros(n), c=cols, cmap='spectral_r', lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed into higher dimensions\n",
    "\n",
    "We embedd this linear point cloud in a 40 dimensional space using a 40 dimensional set of 40 orthogonal bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = np.random.normal(0,1,[m,1])\n",
    "M = np.eye(m)-2*(r.dot(r.T)/r.T.dot(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.c_[x, np.zeros((n,m-1))].dot(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(*X[:,1:3].T, c=x, cmap='spectral', lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "The algorithm PCA finds the linear point cloud and is able to reproduce the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "Xpca = PCA(2).fit_transform(X)\n",
    "plt.scatter(*Xpca.T, c=x, cmap='spectral', lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barnes_hut implementation\n",
    "This implementation of tSNE is taken from https://github.com/danielfrg/tsne.\n",
    "\n",
    "Note the 'artificial' structure in the embeddings. It is not as detailed ('bad') as in the original R implementation of tSNE, but still it introduces some structure and non-linearities.\n",
    "\n",
    "For other implementations in R have a look at https://gist.github.com/mikelove/74bbf5c41010ae1dc94281cface90d32, where others looked at different implementations of tSNE in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3,tight_layout=True,figsize=(15,10))\n",
    "axit = axes.flat\n",
    "for lr in range(6):\n",
    "    ax = next(axit)\n",
    "    Xtsne = tsne.bh_sne(X.copy())\n",
    "    ax.scatter(*Xtsne.T, c=cols, cmap='spectral', lw=0)\n",
    "    ax.set_title('restart: ${}$'.format(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn emplementation\n",
    "\n",
    "Careful with the scikit-learn implementation, the parameters and convergence seems to be unstable.\n",
    "A too high learning rate can lead to a completely random picture. Note that a learning rate of 1000. is the standard in the library.\n",
    "\n",
    "It seems that the scikit-learn implementation suffers from high dependence on the parameters chosen for learning. Beware of this, when using this implementation of tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3,tight_layout=True,figsize=(15,10))\n",
    "axit = axes.flat\n",
    "\n",
    "for perplexity in [30,60]:\n",
    "    for lr in [200, 500, 1000]:\n",
    "        ax = next(axit)\n",
    "        Xtsne = TSNE(perplexity=perplexity, learning_rate=lr, init='pca').fit_transform(X.copy())\n",
    "        ax.scatter(*Xtsne.T, c=cols, cmap='spectral', lw=0)\n",
    "        ax.set_title('perp=${}$, learn-rate=${}$'.format(perplexity, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPLVM\n",
    "\n",
    "We show the performance of GPLVM on the dataset provided. The top right bar plot shows the sensitivity to the dimensions of the embedding. When the sensitivity is high, the dimension is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes\n",
    "\n",
    "fig, axes = plt.subplots(2,3,tight_layout=True,figsize=(15,10))\n",
    "axit = axes.flat\n",
    "for lr in range(6):\n",
    "    ax = next(axit)\n",
    "    m = GPy.models.GPLVM(X.copy(), 2)\n",
    "    m.optimize(messages=1, gtol=0, clear_after_finish=True)\n",
    "    msi = m.get_most_significant_input_dimensions()[:2]\n",
    "    \n",
    "    is_ = m.kern.input_sensitivity().copy()\n",
    "    is_ /= is_.max()\n",
    "    \n",
    "    XBGPLVM = m.X[:,msi] * is_[np.array(msi)]\n",
    "    #m.kern.plot_ARD(ax=ax)\n",
    "    ax.scatter(*XBGPLVM.T, c=cols, cmap='spectral', lw=0)\n",
    "    ax.set_title('restart: ${}$'.format(lr))\n",
    "    ax.set_xlabel('dimension ${}$'.format(msi[0]))\n",
    "    ax.set_ylabel('dimension ${}$'.format(msi[1]))\n",
    "    \n",
    "    a = inset_axes(ax,\n",
    "                    width=\"30%\", # width = 30% of parent_bbox\n",
    "                    height='20%', # height : 1 inch\n",
    "                    loc=1)\n",
    "    sns.barplot(np.array(msi), is_[np.array(msi)], label='input-sens', ax=a)\n",
    "    a.set_title('sensitivity')\n",
    "    a.set_xlabel('dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian GPLVM\n",
    "\n",
    "Last, we want to show the ability of Bayesian GPLVM to not only find the linear imbedding, but also determine the number of necessary dimensions to embedd the data in. We start at a dimensionality of 5 and show that Bayesian GPLVM is capable of identifying one dimension as the significant one and switches off the other ones. See for that the significance of the dimensions in the top right of each subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2,3,tight_layout=True,figsize=(15,10))\n",
    "axit = axes.flat\n",
    "for lr in range(6):\n",
    "    ax = next(axit)\n",
    "    m = GPy.models.BayesianGPLVM(X, 5, num_inducing=25)\n",
    "    m.optimize(messages=1, gtol=0, clear_after_finish=True)\n",
    "    msi = m.get_most_significant_input_dimensions()[:2]\n",
    "    \n",
    "    is_ = m.kern.input_sensitivity()\n",
    "    is_ /= is_.max()\n",
    "    \n",
    "    XBGPLVM = m.X.mean[:,msi] * is_[np.array(msi)]\n",
    "    #m.kern.plot_ARD(ax=ax)\n",
    "    ax.scatter(*XBGPLVM.T, c=cols, cmap='spectral', lw=0)\n",
    "    ax.set_title('restart: ${}$'.format(lr))\n",
    "    ax.set_xlabel('dimension ${}$'.format(msi[0]))\n",
    "    ax.set_ylabel('dimension ${}$'.format(msi[1]))\n",
    "    \n",
    "    a = inset_axes(ax,\n",
    "                    width=\"30%\", # width = 30% of parent_bbox\n",
    "                    height='20%', # height : 1 inch\n",
    "                    loc=1)\n",
    "    sns.barplot(range(m.input_dim), is_, label='input-sens')\n",
    "    a.set_title('sensitivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
